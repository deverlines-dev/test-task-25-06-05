Вопросы:

1. Реализуйте через laravel echo передачу event-а на создание записи в rows
Не особо понятен пункт, нужно объяснение
Может будет так же ошибки передавать в сокет? Они сейчас будут просто логироваться

2. Прогресс парсинга файла хранить в redis (уникальный ключ + количество обработанных строк)
Не особо понятно зачем, и так же вопрос - учитывать пропущенные строки?

3. Реализовать RESTful API контроллер для получения импортированных данных из базы с группировкой по date
Лучше пагинацию добавить, т.к вывод больших данных - это тяжелый процесс для отдающего и принимающего api

4. Вопросы по таблице

id:
- Это уникальное значение?
- Мы его должны сохранять как наш внутренный id или как внешний?

name:
- Тут добавил валидацию на 255 символов
- Предложение: Хорошо было бы поле name разделить на first_name и last_name (вероятно в дальнейшем пригодится, например для приветствия)

date:
- Есть у нас минимальная и максимальная дата?
- Некоторые даты заполнены в формате 08/11/1999, мы их тоже сохраняем?

Есть опечатки в id и датах, например 17.04.1997-, -2084383, 2 003 377
Нужно ли такие данные подгонять под общий формат и сохранять?



Перед выполнением задачи должна быть готова архитектура проекта как самого laravel приложение так и docker

Задача:
Реализация загрузки и обработки Excel файла, содержащий пользователей
Пример таблицы: https://docs.google.com/spreadsheets/...

1. Импорт будет происходить через artisan команду.
Команда должна вызывать сервис для импорта, т.к этот сервис возможно будет использоваться в других местах приложения
Не забываем про транзакции БД.

2. Поискать решение, либо воспользоваться openspout/openspout для импорта таблички.
Если найдётся более подходящее решение, то сначала нужно будет его обсудить.

3. Реализация общего сервиса для обработки таблиц:

Для начала создать общий сервис для обработки таблиц (что бы мы особо не зависили от библиотеки и могли его переиспользовать)
Что он должен уметь:

3.1 Получать пачками (чанками) строки из таблицы

Как пример метода:
public function import(string $path, int $chunkSize = 1000): Generator
{
    // ..
    $reader->open($path);

    foreach ($rows as $number => $row) {
        yield $chunk;
    }

}

3.2 Получать заголовки таблицы
3.3 Написать небольшой тест для этого сервиса

4. Создать миграцию и модель, можно так же репозиторий для данного импорта
В модели добавить ext_id (bigint unsigned, внешний id), id из таблички сохранять нужно в ext_id

Когда сделаешь 3 и 4 пункт, создай и отправь мне мерж, сначала его посмотрим, обсудим.

5. Реализация самого метода импорта данной таблицы:
Нужно получать пачками строки из таблицы (на основе сервиса выше).
Сначала их валидируем (не валидные пропускаем, но логируем).
При валидации учесть, что строки могут быть пустые, даты могут быть заполнены не корректны (например 32 февраля).
Желательно "ручками" валидировать, т.к ларавелевский валидатор в данном случае прожорлив.
Каждый импорт должен иметь уникальный id, например через UUID.
Сохраняем в БД не построчно, а чанками, используем DB, а не ORM
Сервис не должен с каждой итерацией увеличивать кол-во оперативной памяти (можно дебажить в консоль)

После сохранения пачки в БД:
- После каждой части обновить в консоли:
- Сохранить количество обработанных строк в Redis с уникальным ключом (включая UUID и время).
- Отправить broadcast событие (UUID и количество строк) для фронтенда.

Строки не прошедшие валидацию логируем (строка + ошибки в строке)

Сервис можно разбить логично на подсервисы (например для валидации)
Для сервиса делаем unit тест, тестируем как успешные так и не успешные сценарии

6. Реализовать rest api метод для получения этих данных
Метод должен группировать и сортировать по дате
Выводим пагинацию, она работает по датам, например сначала первые 30 дат и с элементами внутри.
Пагинацию пока уточняю, нужна она или нет.
Должна возвращать такой json
[
    {
    date: '2020-01-30',
    items: [
        {
            ext_id: 1,
            name: 'Alina'
        }, ...
        ]
    }, ...
    ]
Так же для него написать тесты, и проверить, что сервис не слишком много ресурсов потребляет

Далее создать мерж и уведомить.